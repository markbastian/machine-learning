{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of entropy as the ability to move around:\n",
    " * Ice - no motion\n",
    " * Water - more\n",
    " * Gas - most \n",
    " \n",
    "Combinatoric example (4 balls of 2 colors):\n",
    " * More homogeneity/rigid less entropy\n",
    " * knowledge and entropy are opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3346791410515946"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entropy equation\n",
    "import math \n",
    "\n",
    "r = 8\n",
    "b = 3\n",
    "y = 2\n",
    "n = float(r + b + y)\n",
    "pr = r / n\n",
    "pb = b / n\n",
    "py = y / n\n",
    "\n",
    "e = 0.0\n",
    "\n",
    "for p in [pr, pb, py]:\n",
    "    e += p * math.log(p, 2)\n",
    "\n",
    "-e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IG = E(p) - avg(E(children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees tend to easily overfit\n",
    " * Random forest: Make n random trees and pick the majority classfication from all of them for a sample\n",
    " \n",
    "Hyperparams:\n",
    " * Max depth - obvious\n",
    " * min samples to split - must have at least n samples\n",
    " * min samples per leaf - min leaf size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard DT Recipe\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Create a classifier/model\n",
    "model = DecisionTreeClassifier()\n",
    "#Example w/hyperparams. These + min_samples_split are most common\n",
    "model = DecisionTreeClassifier(max_depth = 7, min_samples_leaf = 10)\n",
    "\n",
    "#Fit the data\n",
    "model.fit(x_values, y_values)\n",
    "\n",
    "#predict the fit\n",
    "print(model.predict([ [0.2, 0.8], [0.5, 0.4] ]))\n",
    "\n",
    "#Verify the accuracy\n",
    "acc = accuracy_score(y, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n",
    "\n",
    "```python\n",
    "#Drop a feature\n",
    "features_no_names = features_raw.drop(['Name'], axis=1)\n",
    "\n",
    "#One-hot encoding\n",
    "features = pd.get_dummies(features_no_names)\n",
    "\n",
    "#Fill in garbage\n",
    "features = features.fillna(0.0)\n",
    "\n",
    "#Just show the top features\n",
    "display(features.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
